{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "import PyPDF2\n",
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import ConversationChain, LLMChain\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from dotenv import load_dotenv \n",
    "from langchain_core.globals import set_llm_cache\n",
    "from langchain_core.caches import InMemoryCache\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "os.environ['OPENAI_API_KEY']= os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ['PINECONE_API_KEY'] =  os.getenv(\"PINECONE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 6 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 10 0 (offset 0)\n",
      "Ignoring wrong pointing object 12 0 (offset 0)\n",
      "Ignoring wrong pointing object 14 0 (offset 0)\n",
      "Ignoring wrong pointing object 16 0 (offset 0)\n",
      "Ignoring wrong pointing object 18 0 (offset 0)\n",
      "Ignoring wrong pointing object 20 0 (offset 0)\n",
      "Ignoring wrong pointing object 32 0 (offset 0)\n",
      "Ignoring wrong pointing object 34 0 (offset 0)\n"
     ]
    }
   ],
   "source": [
    "#load the pdf file and create text chunks\n",
    "def load_pdf(data):\n",
    "    loader = DirectoryLoader(data,\n",
    "                             glob = \"*.pdf\",\n",
    "                             loader_cls= PyPDFLoader)\n",
    "    documents = loader.load()\n",
    "\n",
    "    return documents\n",
    "\n",
    "#Provide_your_PDF_File\n",
    "extracted_pdf = load_pdf(\"/Documents/Conversational-Chatbot/Data/\") \n",
    "\n",
    "#Text Chunks\n",
    "\n",
    "def text_split(extracted_pdf):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 50)\n",
    "    text_chunks = text_splitter.split_documents(extracted_pdf)\n",
    "    return text_chunks\n",
    "\n",
    "text_chunks = text_split(extracted_pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding model\n",
    "\n",
    "embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# chat completion llm\n",
    "llm = ChatOpenAI(\n",
    "    openai_api_key= os.environ['OPENAI_API_KEY'],\n",
    "    model_name='gpt-3.5-turbo-0125',\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "set_llm_cache(InMemoryCache())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = FAISS.from_documents(text_chunks, embedding_model)\n",
    "db.save_local(\"vector_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever= db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from pydantic import BaseModel, Field, EmailStr\n",
    "from langchain.chains import create_history_aware_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PersonalDetails(BaseModel):\n",
    "    name: str = Field(\n",
    "        None,\n",
    "        description=\"The human name of the user.\",\n",
    "    )\n",
    "    phone_number: int = Field(\n",
    "        None,\n",
    "        description=\"The contact number of the user.\",\n",
    "    )\n",
    "    email: EmailStr = Field(\n",
    "        None,\n",
    "        description=\"an email address that the person associates as theirs\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_what_is_empty(user_peronal_details):\n",
    "    ask_for = []\n",
    "    # Check if fields are empty\n",
    "    for field, value in user_peronal_details.dict().items():\n",
    "        if value in [None, \"\", 0, \"a@gmail.com\"]:  # You can add other 'empty' conditions as per your requirements\n",
    "            print(f\"Field '{field}' is empty.\")\n",
    "            ask_for.append(f'{field}')\n",
    "    return ask_for\n",
    "\n",
    "def add_non_empty_details(current_details: PersonalDetails, new_details: PersonalDetails):\n",
    "    non_empty_details = {k: v for k, v in new_details.dict().items() if v not in [None, \"\"]}\n",
    "    updated_details = current_details.copy(update=non_empty_details)\n",
    "    return updated_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the system prompt for the question-answering task\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer the question \"\n",
    "    \"If you don't know the answer, say that you don't know.\"\n",
    "    \"Use three sentences maximum and keep the answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chatbot_chain(llm, retriever, user_details):\n",
    "    # Step 1: Define the history-aware retriever\n",
    "    retriever_prompt = (\n",
    "        \"Given a chat history and the latest user question which might reference context in the chat history, \"\n",
    "        \"formulate a standalone question which can be understood without the chat history. \"\n",
    "        \"Do NOT answer the question, just reformulate it if needed and otherwise return it as is.\"\n",
    "    )\n",
    "    contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", retriever_prompt),\n",
    "            MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "            (\"human\", \"{input}\"),\n",
    "        ]\n",
    "    )\n",
    "    history_aware_retriever = create_history_aware_retriever(llm, retriever, contextualize_q_prompt)\n",
    "\n",
    "    # Step 2: Define the question-answering chain\n",
    "    qa_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system_prompt),\n",
    "            (\"human\", \"{input}\"),\n",
    "        ]\n",
    "    )\n",
    "    question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "\n",
    "    # Step 3: Define the info-gathering chain\n",
    "    def ask_for_info_chain(ask_for):\n",
    "        first_prompt = ChatPromptTemplate.from_template(\n",
    "            \"You are an assisor book an appotant for question-answering tasks. \"\n",
    "            \"If the user asks you to call intment, there are some things to ask the user in a conversational way. \"\n",
    "            \"You should only ask one question at a time even if you don't get all the info. \"\n",
    "            \"Don't ask as a list! Don't greet the user! Don't say Hi. Explain you need to get some info. \"\n",
    "            \"If the ask_for list is empty, then thank them and ask how you can help them.\\n\\n\"\n",
    "            \"### ask_for list: {ask_for}\\n\\n\"\n",
    "            \"Use three sentences maximum and keep the answer concise. \"\n",
    "            \"If you don't know the answer, say that you don't know.\"\n",
    "        )\n",
    "        return LLMChain(llm=llm, prompt=first_prompt)\n",
    "\n",
    "    # Step 4: Define the overall chatbot logic\n",
    "    def chatbot_chain(input_question, chat_history):\n",
    "        # Step 4a: Use the history-aware retriever to get context\n",
    "        standalone_question = history_aware_retriever.invoke({\"chat_history\": chat_history, \"input\": input_question})\n",
    "\n",
    "        # Step 4b: Answer the question based on documents\n",
    "        answer_from_docs = question_answer_chain.invoke({\"chat_history\": chat_history, \"input\": standalone_question, \n",
    "                                                         \"context\": \"\"})\n",
    "\n",
    "        # Step 4c: Check for personal details if booking is requested\n",
    "        if \"book an appointment\" in input_question.lower():\n",
    "            ask_for = check_what_is_empty(user_details)\n",
    "            if ask_for:\n",
    "                info_gathering_chain = ask_for_info_chain(ask_for)\n",
    "                personal_info_response = info_gathering_chain.run(ask_for=ask_for)\n",
    "                return personal_info_response\n",
    "            else:\n",
    "                return \"Thank you! How else can I assist you?\"\n",
    "\n",
    "        # Return the answer from documents\n",
    "        return answer_from_docs\n",
    "\n",
    "    return chatbot_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_details = PersonalDetails(name=\"\", phone_number= 0, email= 'a@gmail.com')\n",
    "chat_history = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot_chain = create_chatbot_chain(llm, retriever, user_details)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = \"what are the core marketing strategies of Open AI\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI is a prominent player in the field of artificial intelligence, known for its innovative technology and significant impact on the market. The company's marketing strategy focuses on open collaboration, sharing research, and models with the community to position itself as a leader in the industry. OpenAI's emphasis on ethical considerations, transparency, and open research sets a benchmark for others in the AI industry.\n"
     ]
    }
   ],
   "source": [
    "response = chatbot_chain(input_question=user_input, chat_history=chat_history)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Field 'name' is empty.\n",
      "Field 'phone_number' is empty.\n",
      "Field 'email' is empty.\n",
      "What is your name?\n"
     ]
    }
   ],
   "source": [
    "user_input1 = \"i need to book an appointment\"\n",
    "response1 = chatbot_chain(input_question=user_input1, chat_history=chat_history)\n",
    "print(response1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
